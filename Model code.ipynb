{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho7cZHnPQ6N-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**face recognition model**"
      ],
      "metadata": {
        "id": "CMFmH9Bb8Jpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn import preprocessing\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "fQulOqPf8Ryh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connecting from drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPVHJbZC98De",
        "outputId": "a00654c9-452d-4e42-dcdf-99b4cdcbfd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction\n",
        "\n",
        "name = os.listdir('/content/drive/MyDrive/face recognition')\n",
        "x = []\n",
        "y = []\n",
        "for names in name:\n",
        "  filename  =  '/content/drive/MyDrive/face recognition/'+names\n",
        "  d = os.listdir(filename)\n",
        "  for images in d:\n",
        "    img = cv2.imread(filename+\"/\"+images)\n",
        "    grayscale = cv2.cvtColor(img ,cv2.COLOR_BGR2GRAY)\n",
        "    resized_img = cv2.resize(grayscale , (1000 , 800) , interpolation = cv2.INTER_LINEAR)\n",
        "    imgarray = np.ravel(np.asarray(resized_img)) #, dtype=\"object\"\n",
        "    x.append(imgarray)\n",
        "    y.append(names)\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "iU8wqTCa-Ti0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting deep features using pretrained VGG16 model\n",
        "\n",
        "# Load the pre-trained VGG16 model\n",
        "model = VGG16(weights='imagenet')\n",
        "\n",
        "# img_path = '/content/drive/MyDrive/face recognition'\n",
        "\n",
        "name = os.listdir('/content/drive/MyDrive/face recognition')\n",
        "imag = []\n",
        "labels = []\n",
        "for names in name:\n",
        "  filename  =  '/content/drive/MyDrive/face recognition/'+names\n",
        "  d = os.listdir(filename)\n",
        "  for images in d:\n",
        "    img = image.load_img(filename+\"/\"+images,target_size = (224,224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    features = model.predict(img_array)\n",
        "    imag.append(features.flatten())\n",
        "    labels.append(names)\n"
      ],
      "metadata": {
        "id": "Z9ZQq8Uo_WDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yRuESV9_Uit",
        "outputId": "3faf2574-9cbe-41fb-a4a1-11956b4e6376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'harshita', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'vanshika', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'keerti', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'shubh', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'anshika', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra', 'antra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a deep learning model to predict the output\n",
        "\n",
        "def extract_vgg16_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    features = model.predict(img_array)\n",
        "    return features.flatten()\n",
        "\n",
        "def recognize_face(image_path):\n",
        "    # Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Use a pre-trained face detection classifier (e.g., Haarcascades)\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_roi = img[y:y + h, x:x + w]\n",
        "        face_features = extract_vgg16_features(image_path)\n",
        "\n",
        "        # Example: You can perform recognition based on your database of known faces\n",
        "        # Here, we are simply printing the VGG16 predictions for the face features\n",
        "        predictions = decode_predictions(np.array([face_features]))\n",
        "        print(predictions)\n",
        "\n",
        "        # Draw a rectangle around the face\n",
        "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "    # Display the image with rectangles around detected faces\n",
        "    cv2_imshow(img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the path to the image you want to test\n",
        "    image_path = \"/content/drive/MyDrive/face_recognition_output/02.jpg\"\n",
        "\n",
        "    # Perform face recognition\n",
        "    recognize_face(image_path)\n"
      ],
      "metadata": {
        "id": "eASYHsiDCi8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a deep learning model to predict the output\n",
        "\n",
        "model = VGG16(weights='imagenet')\n",
        "face_recognition_model = load_model(\"face_recognition_model.h5\")\n",
        "\n",
        "def extract_vgg16_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    features = model.predict(img_array)\n",
        "    return features.flatten()\n",
        "\n",
        "def recognize_person(image_path):\n",
        "    # Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Use a pre-trained face detection classifier (e.g., Haarcascades)\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_roi = img[y:y + h, x:x + w]\n",
        "        face_features = extract_vgg16_features(image_path)\n",
        "\n",
        "        # Perform face recognition using the pre-trained model\n",
        "        face_features = np.expand_dims(face_features, axis=0)\n",
        "\n",
        "        prediction = face_recognition_model.predict(face_features)\n",
        "\n",
        "        # Example: Display the predicted class (person)\n",
        "        # Modify this part based on your specific use case and dataset\n",
        "        predicted_class = np.argmax(prediction)\n",
        "        print(f\"Predicted Class: {predicted_class}\")\n",
        "\n",
        "        # Draw a rectangle around the face\n",
        "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "    # Display the image with rectangles around detected faces\n",
        "    cv2_imshow(img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the path to the image you want to test\n",
        "    image_path = \"/content/drive/MyDrive/face_recognition_output/02.jpg\"\n",
        "\n",
        "    # Perform face recognition\n",
        "    recognize_person(image_path)\n"
      ],
      "metadata": {
        "id": "Ea0Gm6K9N4Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training my face recognition model on dataset\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set the path to your dataset\n",
        "dataset_path = \"/content/drive/MyDrive/face recognition\"\n",
        "\n",
        "# Set the image dimensions and the number of classes (number of persons)\n",
        "img_width, img_height = 150, 150\n",
        "num_classes = len(os.listdir(dataset_path))\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation to increase the diversity of the training dataset\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(dataset_path,\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "# Train the model\n",
        "model.fit_generator(train_generator, epochs=10)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"face_recognition_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6zsZmG_FDm6",
        "outputId": "26a43325-c42e-44f6-ec9f-e3626db538ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 203 images belonging to 6 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-856c7a96c28d>:39: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator, epochs=10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 12s 1s/step - loss: 16.4216 - accuracy: 0.2118\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 11s 2s/step - loss: 4.0463 - accuracy: 0.2266\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 11s 1s/step - loss: 2.4624 - accuracy: 0.2365\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 11s 1s/step - loss: 1.6205 - accuracy: 0.4236\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 11s 1s/step - loss: 1.4131 - accuracy: 0.4138\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 10s 1s/step - loss: 1.2624 - accuracy: 0.5074\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 17s 2s/step - loss: 1.1732 - accuracy: 0.5320\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 11s 1s/step - loss: 1.0701 - accuracy: 0.5961\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 13s 2s/step - loss: 0.9998 - accuracy: 0.6355\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 12s 1s/step - loss: 0.9160 - accuracy: 0.6650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vgg16_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    features = model.predict(img_array)\n",
        "    return features.flatten()"
      ],
      "metadata": {
        "id": "wP8fAs2CH47W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}